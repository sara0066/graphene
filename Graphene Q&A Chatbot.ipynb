{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphene Q&A Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: \n",
    "\n",
    "[1] Sahota, H. (2023). RAG Evaluation Using LangChain and Ragas. Deci. https://deci.ai/blog/evaluating-rag-pipelines-using-langchain-and-ragas/ \n",
    "\n",
    "[2] Julien Simon (2023). Retrieval-Augmented Generation chatbot, part 1: LangChain, Hugging Face, FAISS, AWS [Video]. YouTube. URL https://www.youtube.com/watch?v=7kDaMz3Xnkw&t=240s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langchain\n",
    "!pip install -U openai\n",
    "!pip install -U ragas\n",
    "!pip install -U arxiv\n",
    "!pip install -U pymupdf\n",
    "!pip install -U chromadb\n",
    "!pip install -U tiktoken\n",
    "!pip install -U accelerate\n",
    "!pip install -U bitsandbytes\n",
    "!pip install -U datasets\n",
    "!pip install -U sentence_transformers\n",
    "!pip install -U FlagEmbedding\n",
    "!pip install -U ninja\n",
    "!pip install -U flash_attn --no-build-isolation\n",
    "!pip install -U tqdm\n",
    "!pip install -U rank_bm25\n",
    "!pip install -U transformers\n",
    "!pip install -U faiss\n",
    "!pip install -U faiss-gpu\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade  boto3 langchain-openai tiktoken python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade  \"amazon-textract-caller>=0.2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install amazon-textract-textractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from getpass import getpass\n",
    "\n",
    "openai.api_key = getpass(\"Please provide your OpenAI Key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading graphene PDF from S3 bucket into notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import AmazonTextractPDFLoader\n",
    "import boto3\n",
    "\n",
    "textract_client = boto3.client(\"textract\", region_name=\"us-east-1\")\n",
    "\n",
    "file_path = \"s3://sagemaker-us-east-1-182943155813/graphene-rag/graphenepdfs_full.pdf\"\n",
    "loader = AmazonTextractPDFLoader(file_path, client=textract_client)\n",
    "all_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len (all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking metadata of corpus\n",
    "\n",
    "for doc in all_docs:\n",
    "  print(doc.metadata) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding embedding model, setting chunk size and overlap and storing it into Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "\n",
    "hf_bge_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={'device': 'cuda'},\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=256,\n",
    "                                               chunk_overlap = 10,\n",
    "                                               length_function=len)\n",
    "\n",
    "docs = text_splitter.split_documents(all_docs)\n",
    "\n",
    "vectorstore = Chroma.from_documents(docs, hf_bge_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking maximum chunk size in vector store\n",
    "\n",
    "print(max([len(chunk.page_content) for chunk in docs]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding base retriever, checking if it works in retrieving relevant documents from vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establishing base retriever. Setting the number of chunks retrieved to be 5 \n",
    "\n",
    "base_retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 5}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_docs = base_retriever.get_relevant_documents(\"What are the steps to produce graphene?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in relevant_docs:\n",
    "  print(doc.page_content)\n",
    "  print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting a system prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This teaches the LLM how to generate a response - how to 'talk'\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"<human>: You are a materials scientist interested in synthesizing graphene with various methods. Answer the question based only on the following context. If you cannot answer the question with the context, please respond with 'I don't know':\n",
    "\n",
    "### CONTEXT\n",
    "{context}\n",
    "\n",
    "### QUESTION\n",
    "Question: {question}\n",
    "\n",
    "\\n\n",
    "\n",
    "<bot>:\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating LLM into the chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import torch\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GenerationConfig, pipeline\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"llmware/dragon-deci-7b-v0\",  # Replace with \"llmware/dragon/mistral-7b-v0\"\n",
    "                                             quantization_config = quantization_config,\n",
    "                                             low_cpu_mem_usage=True,\n",
    "                                             trust_remote_code=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"llmware/dragon-deci-7b-v0\", # Replace with \"llmware/dragon/mistral-7b-v0\"\n",
    "                                          trust_remote_code=True)\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    max_length=4096,\n",
    "    temperature=1e-3,\n",
    "    do_sample=True,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "pipeline = pipeline(\"text-generation\",\n",
    "                    model=model,\n",
    "                    tokenizer=tokenizer,\n",
    "                    max_length=4096,\n",
    "                    temperature=1e-3,\n",
    "                    do_sample=True,\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                    )\n",
    "\n",
    "deci_dragon = HuggingFacePipeline(pipeline=pipeline) # Replace with mistral_dragon for labelling purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a RAG chain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_augmented_qa_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | base_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": prompt | deci_dragon, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying the chatbot with the RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Provide me a step-by-step process to produce high-quality monolayer graphene. Provide the source.\"\n",
    "\n",
    "result = retrieval_augmented_qa_chain.invoke({\"question\" : question})\n",
    "\n",
    "print(result['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the synthetic dataset for LLM (OpenAI) evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "question_schema = ResponseSchema(\n",
    "    name=\"question\",\n",
    "    description=\"a question about the context.\"\n",
    ")\n",
    "\n",
    "question_response_schemas = [\n",
    "    question_schema,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_output_parser = StructuredOutputParser.from_response_schemas(question_response_schemas)\n",
    "\n",
    "format_instructions = question_output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating questions using GPT-3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "question_generation_llm = ChatOpenAI(model=\"gpt-3.5-turbo-1106\")\n",
    "\n",
    "bare_prompt_template = \"{content}\"\n",
    "\n",
    "bare_template = ChatPromptTemplate.from_template(template=bare_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "qa_template = \"\"\"\\\n",
    "You are a materials scientist interested in synthesizing graphene with various methods. For each context, create a question that is specific to the context. Avoid creating generic or general questions.\n",
    "\n",
    "question: a question about the context.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "question\n",
    "\n",
    "context: {context}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template=qa_template)\n",
    "\n",
    "messages = prompt_template.format_messages(\n",
    "    context=docs[0],\n",
    "    format_instructions=format_instructions\n",
    ")\n",
    "\n",
    "question_generation_chain = bare_template | question_generation_llm\n",
    "\n",
    "response = question_generation_chain.invoke({\"content\" : messages})\n",
    "\n",
    "output_dict = question_output_parser.parse(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in output_dict.items():\n",
    "  print(k)\n",
    "  print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieving context from vector store for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "qac_triples = []\n",
    "\n",
    "# Randomly select 10 chunks from the initial number of chunks generated earlier\n",
    "for text in tqdm(random.sample(docs, 10)):\n",
    "  messages = prompt_template.format_messages(\n",
    "      context=text,\n",
    "      format_instructions=format_instructions\n",
    "  )\n",
    "  response = question_generation_chain.invoke({\"content\" : messages})\n",
    "  try:\n",
    "    output_dict = question_output_parser.parse(response.content)\n",
    "  except Exception as e:\n",
    "    continue\n",
    "  output_dict[\"context\"] = text\n",
    "  qac_triples.append(output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qac_triples[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating ground truths using GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_generation_llm = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0)\n",
    "\n",
    "answer_schema = ResponseSchema(\n",
    "    name=\"answer\",\n",
    "    description=\"an answer to the question\"\n",
    ")\n",
    "\n",
    "answer_response_schemas = [\n",
    "    answer_schema,\n",
    "]\n",
    "\n",
    "answer_output_parser = StructuredOutputParser.from_response_schemas(answer_response_schemas)\n",
    "\n",
    "format_instructions = answer_output_parser.get_format_instructions()\n",
    "\n",
    "qa_template = \"\"\"\\\n",
    "You are a materials scientist interested in synthesizing graphene with various methods. For each question and context, create an answer.\n",
    "\n",
    "answer: a answer about the context.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "answer\n",
    "\n",
    "question: {question}\n",
    "context: {context}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template=qa_template)\n",
    "\n",
    "messages = prompt_template.format_messages(\n",
    "    context=qac_triples[0][\"context\"],\n",
    "    question=qac_triples[0][\"question\"],\n",
    "    format_instructions=format_instructions\n",
    ")\n",
    "\n",
    "answer_generation_chain = bare_template | answer_generation_llm\n",
    "\n",
    "response = answer_generation_chain.invoke({\"content\" : messages})\n",
    "\n",
    "output_dict = answer_output_parser.parse(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in output_dict.items():\n",
    "  print(k)\n",
    "  print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for triple in tqdm(qac_triples):\n",
    "  messages = prompt_template.format_messages(\n",
    "      context=triple[\"context\"],\n",
    "      question=triple[\"question\"],\n",
    "      format_instructions=format_instructions\n",
    "  )\n",
    "  response = answer_generation_chain.invoke({\"content\" : messages})\n",
    "  try:\n",
    "    output_dict = answer_output_parser.parse(response.content)\n",
    "  except Exception as e:\n",
    "    continue\n",
    "  triple[\"answer\"] = output_dict[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining questions, context and ground truths into a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "ground_truth_qac_set = pd.DataFrame(qac_triples)\n",
    "\n",
    "ground_truth_qac_set[\"context\"] = ground_truth_qac_set[\"context\"].map(lambda x: str(x.page_content))\n",
    "\n",
    "ground_truth_qac_set = ground_truth_qac_set.rename(columns={\"answer\" : \"ground_truth\"})\n",
    "\n",
    "eval_dataset = Dataset.from_pandas(ground_truth_qac_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding RAGAs metrics into the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    context_relevancy,\n",
    "    answer_correctness,\n",
    "    answer_similarity\n",
    ")\n",
    "\n",
    "from ragas.metrics.critique import harmfulness\n",
    "from ragas import evaluate\n",
    "\n",
    "def create_ragas_dataset(rag_pipeline, eval_dataset):\n",
    "  rag_dataset = []\n",
    "  for row in tqdm(eval_dataset):\n",
    "    answer = rag_pipeline.invoke({\"question\" : row[\"question\"]})\n",
    "    rag_dataset.append(\n",
    "        {\"question\" : row[\"question\"],\n",
    "         \"answer\" : answer[\"response\"],\n",
    "         \"contexts\" : [context.page_content for context in answer[\"context\"]],\n",
    "         \"ground_truths\" : [row[\"ground_truth\"]]\n",
    "         }\n",
    "    )\n",
    "  rag_df = pd.DataFrame(rag_dataset)\n",
    "  rag_eval_dataset = Dataset.from_pandas(rag_df)\n",
    "  return rag_eval_dataset\n",
    "\n",
    "def evaluate_ragas_dataset(ragas_dataset):\n",
    "  result = evaluate(\n",
    "    ragas_dataset,\n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall,\n",
    "        context_relevancy,\n",
    "        answer_correctness,\n",
    "        answer_similarity\n",
    "    ],\n",
    "  )\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inserting the RAG chain with LLM to generate answers and insertting them into the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "basic_qa_ragas_dataset = create_ragas_dataset(retrieval_augmented_qa_chain, eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_qa_ragas_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting the RAG Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_qa_result = evaluate_ragas_dataset(basic_qa_ragas_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting RAGAs performance metrics into a bar graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metrics_with_values(metrics_dict, title='RAG Metrics'):\n",
    "    \"\"\"\n",
    "    Plots a bar chart for metrics contained in a dictionary and annotates the values on the bars.\n",
    "\n",
    "    Args:\n",
    "    metrics_dict (dict): A dictionary with metric names as keys and values as metric scores.\n",
    "    title (str): The title of the plot.\n",
    "    \"\"\"\n",
    "    names = list(metrics_dict.keys())\n",
    "    values = list(metrics_dict.values())\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.barh(names, values, color='skyblue')\n",
    "\n",
    "    # Adding the values on top of the bars\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        plt.text(width + 0.01,  # x-position\n",
    "                 bar.get_y() + bar.get_height() / 2,  # y-position\n",
    "                 f'{width:.4f}',  # value\n",
    "                 va='center')\n",
    "\n",
    "    plt.xlabel('Score')\n",
    "    plt.title(title)\n",
    "    plt.xlim(0, 1)  # Setting the x-axis limit to be from 0 to 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_with_values(basic_qa_result, \"Base Retriever ragas Metrics\") # Add LLM name in title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with advanced retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qa_chain(retriever, primary_qa_llm):\n",
    "  created_qa_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever,\n",
    "     \"question\": itemgetter(\"question\")\n",
    "    }\n",
    "    | RunnablePassthrough.assign(\n",
    "        context=itemgetter(\"context\")\n",
    "      )\n",
    "    | {\n",
    "         \"response\": prompt | primary_qa_llm,\n",
    "         \"context\": itemgetter(\"context\"),\n",
    "      }\n",
    "  )\n",
    "\n",
    "  return created_qa_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parent Document Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=1536)\n",
    "\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=256)\n",
    "\n",
    "vectorstore = Chroma(collection_name=\"split_parents\", embedding_function=hf_bge_embeddings)\n",
    "\n",
    "store = InMemoryStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_document_retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_document_retriever.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_document_retriever_qa_chain = create_qa_chain(parent_document_retriever, deci_dragon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing parent document retriever with query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_document_retriever_qa_chain.invoke({\"question\" : \"Provide me a step-by-step process to produce high-quality monolayer graphene. Provide the source.\"})[\"response\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Making synthetic dataset with parent document retriever in RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdr_qa_ragas_dataset = create_ragas_dataset(parent_document_retriever_qa_chain, eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdr_qa_result = evaluate_ragas_dataset(pdr_qa_ragas_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_with_values(pdr_qa_result, \"Parent Document Retriever ragas Metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=10)\n",
    "\n",
    "docs = text_splitter.split_documents(all_docs)\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(docs)\n",
    "\n",
    "bm25_retriever.k = 5\n",
    "\n",
    "vectorstore = Chroma.from_documents(docs, hf_bge_embeddings)\n",
    "\n",
    "chroma_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, chroma_retriever], weights=[0.42, 0.58]) # Total weights must equal 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_retriever_qa_chain = create_qa_chain(ensemble_retriever, deci_dragon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing ensemble retriever with query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_retriever_qa_chain.invoke({\"question\" : \"Provide me a step-by-step process to produce high-quality monolayer graphene. Provide the source.\"})[\"response\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Making synthetic dataset with ensemble retriever in RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_qa_ragas_dataset = create_ragas_dataset(ensemble_retriever_qa_chain, eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_qa_result = evaluate_ragas_dataset(ensemble_qa_ragas_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_with_values(ensemble_qa_result, \"Ensemble Retriever ragas Metrics\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
